{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data = pd.read_csv('DATA/SMSSpamCollection.txt', sep='\\t', header=None, names=['Target', 'Email'])\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1    ham                        ok lar... joke wif u oni...\n",
       "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3    ham          u dun say earli hor... u c alreadi say...\n",
       "4    ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "# stopword filtering\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "#stemming\n",
    "st = PorterStemmer()\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join ([st.stem(word) for word in x.split()]))\n",
    "#lemmatize\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join ([Word(word).lemmatize() for word in x.split()]))\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPzElEQVR4nO3df6zd9V3H8edrLWPoRCFcEHvZypKaCAw3ucEmM3EZOmowlsQwu0RpIkkTgmbqkq2YLWZqE/SPxWAG2uhC2a+miSPtNtiGVWaMbOyyzZXyIzSDQW1Hy5ZlzMS6dm//OB/csT3cewrtOfV8no/km+/3+z6fz/d8Tnru6377+X7PuakqJEl9eNW0ByBJmhxDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI2OFfpKnk+xJ8rUki612fpL7kzzZ1ucNtb81yb4kTyS5dqh+VTvOviS3J8mpf0mSpJeSce7TT/I0sFBVzw/V/hL4TlXdlmQzcF5VvTfJZcAngKuBnwH+EfjZqjqW5CHgXcAXgXuB26vqvqWe+4ILLqjVq1e/rBcnSb16+OGHn6+quePrK1/BMdcDb23b24AHgPe2+vaqOgI8lWQfcHX7xXFuVT0IkORu4HpgydBfvXo1i4uLr2CYktSfJN8cVR93Tr+Azyd5OMmmVruoqg4CtPWFrb4KeHao7/5WW9W2j69LkiZk3DP9t1TVgSQXAvcneXyJtqPm6WuJ+okHGPxi2QTwute9bswhSpKWM9aZflUdaOtDwD0M5uufS3IxQFsfas33A5cMdZ8HDrT6/Ij6qOfbWlULVbUwN3fClJQk6WVaNvST/HiSn3hxG3g78AiwC9jYmm0EdrbtXcCGJGcnuRRYAzzUpoBeSLK23bVz41AfSdIEjDO9cxFwT7u7ciXw8ar6bJIvAzuS3AQ8A9wAUFV7k+wAHgWOArdU1bF2rJuBu4BzGFzAXfIiriTp1Brrls1pWlhYKO/ekaSTk+Thqlo4vu4nciWpI4a+JHXklXw4S0NWb/7MtIcwM56+7bppD0GaWZ7pS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkbFDP8mKJF9N8um2f36S+5M82dbnDbW9Ncm+JE8kuXaoflWSPe2x25Pk1L4cSdJSTuZM/13AY0P7m4HdVbUG2N32SXIZsAG4HFgH3JFkRetzJ7AJWNOWda9o9JKkkzJW6CeZB64D/m6ovB7Y1ra3AdcP1bdX1ZGqegrYB1yd5GLg3Kp6sKoKuHuojyRpAsY90/8r4D3AD4dqF1XVQYC2vrDVVwHPDrXb32qr2vbxdUnShCwb+kl+HThUVQ+PecxR8/S1RH3Uc25Ksphk8fDhw2M+rSRpOeOc6b8F+I0kTwPbgbcl+SjwXJuyoa0Ptfb7gUuG+s8DB1p9fkT9BFW1taoWqmphbm7uJF6OJGkpy4Z+Vd1aVfNVtZrBBdp/qqrfBnYBG1uzjcDOtr0L2JDk7CSXMrhg+1CbAnohydp2186NQ30kSROw8hX0vQ3YkeQm4BngBoCq2ptkB/AocBS4paqOtT43A3cB5wD3tUWSNCEnFfpV9QDwQNv+NnDNS7TbAmwZUV8ErjjZQUqSTg0/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI8uGfpLXJHkoyb8n2ZvkA61+fpL7kzzZ1ucN9bk1yb4kTyS5dqh+VZI97bHbk+T0vCxJ0ijjnOkfAd5WVT8PvAlYl2QtsBnYXVVrgN1tnySXARuAy4F1wB1JVrRj3QlsAta0Zd2peymSpOUsG/o18P22e1ZbClgPbGv1bcD1bXs9sL2qjlTVU8A+4OokFwPnVtWDVVXA3UN9JEkTMNacfpIVSb4GHALur6ovARdV1UGAtr6wNV8FPDvUfX+rrWrbx9clSRMyVuhX1bGqehMwz+Cs/Yolmo+ap68l6iceINmUZDHJ4uHDh8cZoiRpDCd1905VfRd4gMFc/HNtyoa2PtSa7QcuGeo2Dxxo9fkR9VHPs7WqFqpqYW5u7mSGKElawjh378wl+am2fQ7wK8DjwC5gY2u2EdjZtncBG5KcneRSBhdsH2pTQC8kWdvu2rlxqI8kaQJWjtHmYmBbuwPnVcCOqvp0kgeBHUluAp4BbgCoqr1JdgCPAkeBW6rqWDvWzcBdwDnAfW2RJE3IsqFfVV8H3jyi/m3gmpfoswXYMqK+CCx1PUCSdBr5iVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHVk29JNckuSfkzyWZG+Sd7X6+UnuT/JkW5831OfWJPuSPJHk2qH6VUn2tMduT5LT87IkSaOMc6Z/FHh3Vf0csBa4JcllwGZgd1WtAXa3fdpjG4DLgXXAHUlWtGPdCWwC1rRl3Sl8LZKkZSwb+lV1sKq+0rZfAB4DVgHrgW2t2Tbg+ra9HtheVUeq6ilgH3B1kouBc6vqwaoq4O6hPpKkCTipOf0kq4E3A18CLqqqgzD4xQBc2JqtAp4d6ra/1Va17ePrkqQJGTv0k7wW+AfgD6rqe0s1HVGrJeqjnmtTksUki4cPHx53iJKkZYwV+knOYhD4H6uqT7byc23KhrY+1Or7gUuGus8DB1p9fkT9BFW1taoWqmphbm5u3NciSVrGOHfvBPh74LGq+uDQQ7uAjW17I7BzqL4hydlJLmVwwfahNgX0QpK17Zg3DvWRJE3AyjHavAX4HWBPkq+12h8DtwE7ktwEPAPcAFBVe5PsAB5lcOfPLVV1rPW7GbgLOAe4ry2SpAlZNvSr6l8ZPR8PcM1L9NkCbBlRXwSuOJkBSpJOHT+RK0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjy4Z+kg8nOZTkkaHa+UnuT/JkW5839NitSfYleSLJtUP1q5LsaY/dniSn/uVIkpYyzpn+XcC642qbgd1VtQbY3fZJchmwAbi89bkjyYrW505gE7CmLccfU5J0mi0b+lX1L8B3jiuvB7a17W3A9UP17VV1pKqeAvYBVye5GDi3qh6sqgLuHuojSZqQlzunf1FVHQRo6wtbfRXw7FC7/a22qm0fX5ckTdCpvpA7ap6+lqiPPkiyKcliksXDhw+fssFJUu9ebug/16ZsaOtDrb4fuGSo3TxwoNXnR9RHqqqtVbVQVQtzc3Mvc4iSpOO93NDfBWxs2xuBnUP1DUnOTnIpgwu2D7UpoBeSrG137dw41EeSNCErl2uQ5BPAW4ELkuwH/gS4DdiR5CbgGeAGgKram2QH8ChwFLilqo61Q93M4E6gc4D72iJJmqBlQ7+q3vkSD13zEu23AFtG1BeBK05qdJKkU8pP5EpSRwx9SeqIoS9JHTH0Jakjhr4kdWTZu3ck/f+2evNnpj2EmfL0bddNewiviGf6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6sjEQz/JuiRPJNmXZPOkn1+SejbR0E+yAvgQ8GvAZcA7k1w2yTFIUs8mfaZ/NbCvqr5RVf8NbAfWT3gMktStSYf+KuDZof39rSZJmoCVE36+jKjVCY2STcCmtvv9JE+c1lH14wLg+WkPYjn5i2mPQFPi+/PUev2o4qRDfz9wydD+PHDg+EZVtRXYOqlB9SLJYlUtTHsc0ii+Pydj0tM7XwbWJLk0yauBDcCuCY9Bkro10TP9qjqa5PeAzwErgA9X1d5JjkGSejbp6R2q6l7g3kk/rwCnzHRm8/05Aak64TqqJGlG+TUMktQRQ1+SOmLoS1JHJn4hV5OX5EpgNUP/3lX1yakNSOJ/v4vrOk58b35wWmPqgaE/45J8GLgS2Av8sJULMPQ1bZ8C/gvYw4/emzrNDP3Zt7aq/CZTnYnmq+rKaQ+iN87pz74H/fpqnaHuS/L2aQ+iN57pz75tDIL/W8ARBl96V55h6QzwReCeJK8CfsCP3pvnTndYs80PZ824JPuAP+K4edOq+ubUBiUBSb4BXA/sKYNoYjzTn33PVJVfaqcz0ZPAIwb+ZBn6s+/xJB9ncKfEkReL3rKpM8BB4IEk9/F/35vesnkaGfqz7xwGP1DDF8y8ZVNngqfa8uq2aAKc05ekjnimP+OSvAa4CbgceM2L9ar63akNSgKSzAHv4cT35tumNqgOeJ/+7PsI8NPAtcAXGPyJyhemOiJp4GPA48ClwAeApxn8dT2dRk7vzLgkX62qNyf5elVdmeQs4HOeTWnakjxcVVe9+N5stS9U1S9Pe2yzzOmd2feDtv5ukiuAbzH4gitp2l58bx5Mch1wgMH/RHUaGfqzb2uS84D3Mfgj9K8F3j/dIUkA/HmSnwTeDfw1cC7wh9Md0uxzemfGJTkb+E0GZ/dntXJV1Z9ObVCSpsYLubNvJ7AeOAp8vy3/OdURSUCSNyT5VJLnkxxKsjPJG6Y9rlnnmf6MS/JIVV0x7XFIx0vyReBDwCdaaQPw+1X1i9Mb1ezzTH/2/VuSN057ENIIqaqPVNXRtnyUwafFdRp5pj+jkuxh8AO0ElgDfAO/WllnkCS3Ad8FtjN4r/4WcDaDs3+q6jtTG9wMM/RnVJLXL/W4X62saUvy1NDui0GUF/eryvn908DQlzQVSd4BfLaqvpfk/cAvAH9WVV+Z8tBmmnP6kqblfS3wfwn4VeAu4M7pDmn2GfqSpuVYW18H/E1V7cSvWD7tDH1J0/IfSf4WeAdwb/sgoZl0mjmnL2kqkvwYsI7B38h9MsnFwBur6vNTHtpMM/QlqSP+V0qSOmLoS1JHDH1J6oihL0kdMfQlqSP/A1WkI5CdfzCmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=Email_Data['Target'].value_counts()\n",
    "result.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train: (4457, 2)\n",
      "shape of test: (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "#Train Test Split\n",
    "train, test = train_test_split(Email_Data[['Email', 'Target']] , test_size=0.2)\n",
    "print(f\"shape of train: {train.shape}\")\n",
    "print(f\"shape of test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# tokenize\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train.Email)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7586 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_data: (4457, 300)\n",
      "shape of test_data: (1115, 300)\n"
     ]
    }
   ],
   "source": [
    "# get only the top frequent words on train\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# get only the top frequent words on test\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"shape of train_data: {train_data.shape}\")\n",
    "print(f\"shape of test_data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n",
      "Unique Class For Train: (array([0, 1]), array([3874,  583], dtype=int64))\n",
      "Unique Class For Test: (array([0, 1]), array([951, 164], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Count Of Unique Class\n",
    "print(le.classes_)\n",
    "print(f\"Unique Class For Train: {np.unique(train_labels, return_counts=True)}\")\n",
    "print(f\"Unique Class For Test: {np.unique(test_labels, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4457, 300)\n",
      "Shape of label tensor: (4457, 2)\n",
      "Shape of label tensor: (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "# changing data types\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print(f'Shape of data tensor: {train_data.shape}')\n",
    "print(f'Shape of label tensor: {labels_train.shape}')\n",
    "print(f'Shape of label tensor: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 15s 200ms/step - loss: 0.3808 - accuracy: 0.8481 - val_loss: 0.6164 - val_accuracy: 0.8529\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 13s 192ms/step - loss: 0.1506 - accuracy: 0.9468 - val_loss: 1.0792 - val_accuracy: 0.8529\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 14s 198ms/step - loss: 0.0610 - accuracy: 0.9838 - val_loss: 1.3873 - val_accuracy: 0.8529\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 14s 197ms/step - loss: 0.0335 - accuracy: 0.9921 - val_loss: 1.3116 - val_accuracy: 0.8529\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 14s 206ms/step - loss: 0.0210 - accuracy: 0.9946 - val_loss: 1.2339 - val_accuracy: 0.8529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb6db66850>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train ,validation_data=(test_data, labels_test), batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 19ms/step - loss: 1.2339 - accuracy: 0.8529\n",
      "Accuracy Convolution: 85.2914810180664\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.evaluate(test_data, labels_test) \n",
    "print(f\"Accuracy Convolution: {evaluate[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE RNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "279/279 [==============================] - 26s 89ms/step - loss: 0.4840 - accuracy: 0.8981 - val_loss: 0.4059 - val_accuracy: 0.9022\n",
      "Epoch 2/5\n",
      "279/279 [==============================] - 25s 88ms/step - loss: 0.2872 - accuracy: 0.9605 - val_loss: 0.3173 - val_accuracy: 0.9256\n",
      "Epoch 3/5\n",
      "279/279 [==============================] - 25s 89ms/step - loss: 0.1780 - accuracy: 0.9814 - val_loss: 0.2822 - val_accuracy: 0.9184\n",
      "Epoch 4/5\n",
      "279/279 [==============================] - 25s 89ms/step - loss: 0.1122 - accuracy: 0.9906 - val_loss: 0.2519 - val_accuracy: 0.9238\n",
      "Epoch 5/5\n",
      "279/279 [==============================] - 25s 88ms/step - loss: 0.0789 - accuracy: 0.9946 - val_loss: 0.2580 - val_accuracy: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb6e083be0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train ,validation_data=(test_data, labels_test), batch_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 16ms/step - loss: 0.2580 - accuracy: 0.9139\n",
      "Accuracy Convolution: 91.39013290405273\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.evaluate(test_data, labels_test) \n",
    "print(f\"Accuracy Convolution: {evaluate[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9919325e-01, 8.0679148e-04],\n",
       "       [9.9945825e-01, 5.4177974e-04],\n",
       "       [9.9896061e-01, 1.0394339e-03],\n",
       "       ...,\n",
       "       [9.9598444e-01, 4.0155533e-03],\n",
       "       [9.9784601e-01, 2.1540474e-03],\n",
       "       [9.9938500e-01, 6.1496755e-04]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.93138244 0.77419355]\n",
      "recall: [0.97055731 0.58536585]\n",
      "fscore: [0.95056643 0.66666667]\n",
      "support: [951 164]\n",
      "############################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       951\n",
      "           1       0.77      0.59      0.67       164\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1115\n",
      "   macro avg       0.85      0.78      0.81      1115\n",
      "weighted avg       0.91      0.91      0.91      1115\n",
      " samples avg       0.91      0.91      0.91      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "279/279 [==============================] - 40s 136ms/step - loss: 0.1103 - accuracy: 0.9652 - val_loss: 0.3568 - val_accuracy: 0.9874\n",
      "Epoch 2/5\n",
      "279/279 [==============================] - 37s 133ms/step - loss: 0.0111 - accuracy: 0.9978 - val_loss: 0.0826 - val_accuracy: 0.9865\n",
      "Epoch 3/5\n",
      "279/279 [==============================] - 38s 135ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0825 - val_accuracy: 0.9848\n",
      "Epoch 4/5\n",
      "279/279 [==============================] - 37s 133ms/step - loss: 6.1157e-04 - accuracy: 0.9998 - val_loss: 0.0842 - val_accuracy: 0.9848\n",
      "Epoch 5/5\n",
      "279/279 [==============================] - 39s 141ms/step - loss: 2.4325e-04 - accuracy: 1.0000 - val_loss: 0.1214 - val_accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb6ff8feb0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train ,validation_data=(test_data, labels_test), batch_size=16, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 2s 44ms/step - loss: 0.1214 - accuracy: 0.9865\n",
      "Accuracy Convolution: 98.65471124649048\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.evaluate(test_data, labels_test) \n",
    "print(f\"Accuracy Convolution: {evaluate[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000000e+00, 6.667492e-22],\n",
       "       [1.000000e+00, 7.282981e-19],\n",
       "       [1.000000e+00, 8.028914e-12],\n",
       "       ...,\n",
       "       [1.000000e+00, 3.062356e-11],\n",
       "       [9.999999e-01, 9.280084e-08],\n",
       "       [1.000000e+00, 9.604310e-13]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.98447205 1.        ]\n",
      "recall: [1.         0.90853659]\n",
      "fscore: [0.99217527 0.95207668]\n",
      "support: [951 164]\n",
      "############################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       951\n",
      "           1       1.00      0.91      0.95       164\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1115\n",
      "   macro avg       0.99      0.95      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      " samples avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,predicted_lstm.round()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
